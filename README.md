# Monte Carlo and Quasi-Monte Carlo Methods for Ï€ Estimation

This repository contains implementations of various Monte Carlo (MC) and Quasi-Monte Carlo (QMC) variance reduction techniques for estimating the value of Ï€. All implementations use the classic circle-inscribed-in-square method.

## ğŸ“Š Overview

The project demonstrates different sampling strategies and variance reduction techniques to improve the accuracy and efficiency of Monte Carlo simulations. Each implementation estimates Ï€ by randomly sampling points in a unit square and counting how many fall within a quarter circle.

### Mathematical Background

The basic principle: A circle with radius 1 has area Ï€, and a square with side 2 has area 4. The ratio of areas is Ï€/4. By sampling random points in a unit square [0,1]Ã—[0,1] and checking if they fall within the quarter circle (xÂ² + yÂ² â‰¤ 1), we can estimate:

$$\pi \approx 4 \times \frac{\text{points inside circle}}{\text{total points}}$$

---

## ğŸ“ Files Description

### 1. `simplified_mc.py` - Basic Monte Carlo
**Purpose**: Baseline implementation of standard Monte Carlo method.

**Features**:
- Pure random sampling
- Simple circle inclusion check using distance formula
- Performance timing

**Algorithm**:
1. Generate N random points (x, y) in [0, 1]Ã—[0, 1]
2. Check if âˆš(xÂ² + yÂ²) â‰¤ 1
3. Estimate Ï€ = 4 Ã— (points_inside / N)

**Usage**:
```bash
python simplified_mc.py
```

---

### 2. `antithetic_variates_mc.py` - Antithetic Variates Method
**Purpose**: Reduce variance using negatively correlated samples.

**Features**:
- Generates complementary pairs: (x, y) and (1-x, 1-y)
- Reduces variance by averaging negatively correlated estimates
- More efficient than standard MC with same number of samples

**Algorithm**:
1. For each iteration, generate one random point (x, y)
2. Also evaluate its antithetic pair (1-x, 1-y)
3. Both points contribute to the estimate
4. Variance reduction through negative correlation

**Theoretical Advantage**:
- If f(U) and f(1-U) are negatively correlated, Var[(f(U) + f(1-U))/2] < Var[f(U)]

**Usage**:
```bash
python antithetic_variates_mc.py
```

---

### 3. `nmc_vs_antithetic_mc.py` - Comparison Study
**Purpose**: Direct comparison between Normal MC and Antithetic Variates MC.

**Features**:
- Runs both methods with same N
- Side-by-side performance and accuracy comparison
- Timing analysis for both approaches

**Metrics Compared**:
- Estimation accuracy (closeness to Ï€ â‰ˆ 3.14159...)
- Execution time
- Variance (across multiple runs)

**Usage**:
```bash
python nmc_vs_antithetic_mc.py
```

---

### 4. `nmc_vs_stratification.py` - Stratified Sampling
**Purpose**: Compare Normal MC with Stratified Sampling technique.

**Features**:
- Divides sampling space into K strata (default K=4)
- Ensures uniform coverage across the domain
- Reduces variance by eliminating clustering

**Algorithm (Stratified Sampling)**:
1. Divide x-axis into K equal intervals [j/K, (j+1)/K]
2. Sample N/K points from each stratum
3. Sample x uniformly in [j/K, (j+1)/K] and y in [0, 1]
4. Aggregate results across all strata

**Theoretical Advantage**:
- Guarantees coverage of entire space
- Reduces variance compared to pure random sampling
- Particularly effective when integrand varies smoothly

**Usage**:
```bash
python nmc_vs_stratification.py
```

---

### 5. `nmc_vs_control_variate_mc.py` - Control Variates Method
**Purpose**: Use a correlated function with known expectation to reduce variance.

**Features**:
- Control function: g(x, y) = 1 - (xÂ² + yÂ²)
- Known expectation: E[g] = 1/3
- Adjusts estimate using correlation between indicator and control function

**Algorithm**:
1. For each sample, compute both:
   - I = 1 if xÂ² + yÂ² â‰¤ 1, else 0 (indicator function)
   - g = 1 - (xÂ² + yÂ²) (control variate)
2. Calculate covariance Cov(I, g) and variance Var(g)
3. Compute optimal coefficient c = -Cov(I, g) / Var(g)
4. Adjusted estimate: E[I] + c(E[g] - Î¼), where Î¼ = 1/3

**Theoretical Advantage**:
- Variance reduction: Var(I + c(g - Î¼)) < Var(I) when I and g are correlated
- Uses known information to improve estimate

**Usage**:
```bash
python nmc_vs_control_variate_mc.py
```

---

### 6. `merged_code.py` - Monte Carlo vs Quasi-Monte Carlo
**Purpose**: Compare pseudo-random (MC) with low-discrepancy (QMC) sequences.

**Features**:
- **MC**: Standard pseudo-random sampling
- **QMC**: Halton sequence (bases 2 and 3)
- Demonstrates superiority of low-discrepancy sequences

**Halton Sequence**:
The Halton sequence generates points that are more evenly distributed than random numbers:
```
Halton(i, base) = Î£ (aâ‚– / baseáµâºÂ¹)
where i = Î£ aâ‚– Ã— baseáµ (base representation of i)
```

**Algorithm**:
1. **MC**: Generate points using random.random()
2. **QMC**: Generate points using Halton sequence with bases 2 (for x) and 3 (for y)
3. Both methods evaluate same number of points

**Theoretical Advantage**:
- QMC error: O(log(N)áµˆ / N) vs MC error: O(1/âˆšN)
- Better space-filling properties
- Faster convergence for smooth integrands

**Usage**:
```bash
python merged_code.py
```

---

## ğŸ¯ Variance Reduction Techniques Summary

| Technique | Key Idea | Variance Reduction | Computational Cost |
|-----------|----------|-------------------|-------------------|
| **Antithetic Variates** | Use complementary samples | Moderate | Low (+0%) |
| **Stratified Sampling** | Divide domain into regions | High | Low (+5%) |
| **Control Variates** | Use correlated function | High | Medium (+20%) |
| **Quasi-Monte Carlo** | Low-discrepancy sequences | Very High | Low (+0%) |

---

## ğŸš€ Getting Started

### Prerequisites
```bash
python 3.x
# Standard library only - no external dependencies!
```

### Running the Experiments

1. **Basic estimation**:
   ```bash
   python simplified_mc.py
   # Input: 100000
   ```

2. **Compare variance reduction techniques**:
   ```bash
   python nmc_vs_antithetic_mc.py
   python nmc_vs_stratification.py
   python nmc_vs_control_variate_mc.py
   python merged_code.py
   ```

### Recommended Sample Sizes
- Quick test: N = 10,000
- Standard: N = 100,000
- High accuracy: N = 1,000,000
- Research: N = 10,000,000+

---

## ğŸ“ˆ Expected Results

For N = 100,000 samples (approximate):

| Method | Typical Error | Relative Speed |
|--------|--------------|----------------|
| Normal MC | Â±0.01 | 1.0x |
| Antithetic Variates | Â±0.007 | 1.0x |
| Stratified (K=4) | Â±0.005 | 1.05x |
| Control Variates | Â±0.004 | 1.2x |
| QMC (Halton) | Â±0.002 | 1.0x |

*Note: Actual results vary due to randomness*

---

## ğŸ“š Key Concepts Demonstrated

### 1. **Law of Large Numbers**
As N â†’ âˆ, the Monte Carlo estimate converges to the true value.

### 2. **Central Limit Theorem**
Error decreases as O(1/âˆšN) for standard MC.

### 3. **Variance Reduction**
Using problem structure and mathematical properties to reduce estimation variance without increasing sample size.

### 4. **Low-Discrepancy Sequences**
QMC methods achieve better convergence rates O(log(N)/N) by using deterministic, evenly-distributed points.

---

## ğŸ”¬ Course Context

**Course**: Mathematical Methods in Computer Science (MMCS)  
**Topic**: Monte Carlo and Quasi-Monte Carlo Methods  
**Semester**: 3  

This project demonstrates practical implementations of variance reduction techniques for numerical integration and demonstrates the trade-offs between different sampling strategies.

---

## ğŸ“Š Performance Analysis Tips

To properly evaluate these methods:

1. **Run multiple times**: MC methods are stochastic
2. **Use same N**: Fair comparison requires same sample size
3. **Measure variance**: Run each method 100+ times and compute std deviation
4. **Plot convergence**: Show error vs. sample size on log-log plot
5. **Time complexity**: Consider computational overhead of variance reduction

---

## ğŸ“ Learning Outcomes

By studying this code, you will understand:

- âœ… Basic Monte Carlo integration
- âœ… Variance reduction techniques and their theoretical foundations
- âœ… Trade-offs between accuracy and computational cost
- âœ… Difference between pseudo-random and quasi-random sequences
- âœ… Practical implementation of statistical methods

---

## ğŸ“– References

1. **Antithetic Variates**: Hammersley & Morton (1956)
2. **Stratified Sampling**: Cochran (1977)
3. **Control Variates**: Lavenberg & Welch (1981)
4. **Quasi-Monte Carlo**: Halton (1960), Sobol (1967)

---

## ğŸ¤ Author

**Vaibhav Mangroliya**  
SEM 3, MMCS Course

---

## ğŸ“ License

Educational project for academic purposes.

---

## ğŸ’¡ Future Enhancements

Potential extensions to explore:
- [ ] Latin Hypercube Sampling
- [ ] Importance Sampling
- [ ] Sobol sequences (another QMC method)
- [ ] Convergence plots and statistical analysis
- [ ] Multi-dimensional integration examples
- [ ] Adaptive stratification
- [ ] Parallel implementation using multiprocessing

---

## ğŸ› Notes

- All implementations use the same basic circle-in-square method
- Timing may vary based on system performance
- For research-grade results, use N â‰¥ 10â¶
- QMC methods work best for smooth, low-dimensional integrands

---

**Last Updated**: October 2025
